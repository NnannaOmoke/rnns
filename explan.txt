A recurrent perceptron will, instead of feeding it's adjusted biases forward, will feed it back to itself
2 issues arise when rnns increase in size, vanishing gradients, and exploding gradients
LSTMs -> Long Short Term Memory perceptrons. LSTMs posses long and short term memory inputs and outputs, so as to alleviate the problems that rnns have in remembering biases assigned to earlier members of the sequence
Gated Reccurent Units